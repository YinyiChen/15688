{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #WILL_YOU_FOLLOW_OBAMA_ON_TWITTER?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of celebrities using Twitter, who are followed by tens of millions of general users. Whether a specific user will follow a particular celebrity is an interesting question to ask.\n",
    "\n",
    "In this project, the team aims at answering the question of whether a looks into predicting whether a given user is following Barack Obama (@BarackObama). Two different learning models are tried: Collaborative Filtering and Bag-of-words model. Collaborative Filtering did not work quite well in our case, but after switching gear, bag-of-words seems to work much better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Twitter API and Tweepy\n",
    "\n",
    "In this project we mainly used tweepy as a wrapper of the Twitter API. Most queries we made were for user followers, friends, and tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from twython import Twython\n",
    "\n",
    "\n",
    "consumer_key = '<CONSUMER_KEY>'\n",
    "consumer_secret = '<CONSUMER_SECRET>'\n",
    "access_token = '<ACCESS_TOKEN>'\n",
    "access_secret = '<ACCESS_SECRET>'\n",
    "auth = OAuthHandler(consumer_key=consumer_key, consumer_secret=consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "api = tweepy.API(auth) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team found that the behavior of a user following another user is in some way similar to how movie ratings happen for MovieLens dataset. The difference is that, instead of a floating number from 1 to 5 for a rating, the unidirectional relationship of follow/not follow only has a binary value. It would be interesting to observe how this property would affect the performance of alternating least squares method when it’s applied. And as a possible way to close the gap between the two scenarios, a “fondness” score could be added for a follower to one of the target he or she is following, indicating how much of a fan the follower is to the target. For example, if a user has retweeted or liked a lot of posts of the target, the fondness score might then be relatively high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Collecting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First,the team chose 100 popular twitter accounts based on Twitter statistics and used BeautifulSoup to get a list these 100 accounts’ screen_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "web = \"http://twittercounter.com/pages/100?vt=1&utm_expid=102679131-111.l9w6V73qSUykZciySuTZuA.1&utm_referrer=https%3A%2F%2Fwww.google.com%2F\"\n",
    "\n",
    "page = urllib2.urlopen(web)\n",
    "soup = BeautifulSoup(page)\n",
    "span_list = soup.find_all(\"span\", {\"itemprop\":\"alternateName\"})\n",
    "# list of screen name of top 100 Twitter celebrity\n",
    "celebrities = [] \n",
    "for each in span_list:\n",
    "    celebrities.append(str(each.text)[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we hope to get 1 follower from each celebrity's follower group, by simply selecting the most recent follower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_n_recent_follower_ids(id, n):\n",
    "    \"\"\" Retrieve the most recent n followers from a given Twitter id or screen name. \"\"\"\n",
    "    \n",
    "    followers = []\n",
    "    for i, follower_list in enumerate(tweepy.Cursor(api.followers_ids, id).pages()):\n",
    "        if len(followers) >= n:\n",
    "            break\n",
    "        followers.append(follower_list)\n",
    "    return followers\n",
    "\n",
    "\n",
    "def generate_user_id_list (id_list,n):\n",
    "    \"\"\" Generate the entire set of users that unions the returned list of running the retrieve_n_follower_ids \n",
    "    for each celebrity account\"\"\"\n",
    "    \n",
    "    users = []\n",
    "    for id in id_list:\n",
    "        try:\n",
    "            new_users = retrieve_n_follower_ids(id, n)\n",
    "        except tweepy.TweepError:\n",
    "            time.sleep(60*15)\n",
    "            new_users = retrieve_n_follower_ids(id, n)\n",
    "        users += new_users\n",
    "        \n",
    "    return set(users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = generate_user_id_list (celebrities, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Generate User-celebrity matrix\n",
    "\n",
    "Now that both user and celebrity arrays are ready, the following step is to generate the m by n matrix (m is number of total general users and n the number of celebrities collected). matrix[i][j] indicates whether user i is currently following celebrity j. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def genenerate_user_celebrity_matrix(users, celebrities): # both are lists of Twitter IDs\n",
    "    matrix = np.zeros((len(users), len(celebrities)))\n",
    "    celebrity_set = set(celebrities)\n",
    "    for i in range(len(users)):\n",
    "        try:\n",
    "            friends = get_friends(users[i]) \n",
    "        except tweepy.TweepError:\n",
    "            time.sleep(60*15)\n",
    "            friends = get_friends(users[i])    \n",
    "        for j in range(len(celebrities)):\n",
    "            matrix[i][j] = celebrities[j] in friends\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix = genenerate_user_celebrity_matrix(users, celebrities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training and Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def process(following, P):\n",
    "    \"\"\" Given a dataframe of following and a random permutation, split the data into a training \n",
    "        and a testing set, in matrix form. \n",
    "        \n",
    "        Args: \n",
    "            following (2D numpy array) : a 2D numpy array of following \n",
    "            P (numpy 1D array) : random permutation vector\n",
    "            \n",
    "        Returns: \n",
    "            (X_tr, X_te)  : training and testing splits of the following matrix (both \n",
    "                                         numpy 2D arrays) \n",
    "    \"\"\"\n",
    "    l= len(following)   \n",
    "    train_length = int(math.floor(l*0.9))\n",
    "    train_P = P[0:train_length]\n",
    "    test_P = P[train_length:]\n",
    "    \n",
    "    new_p1 = np.zeros(following.shape[1])\n",
    "    new_p2= np.zeros(following.shape[1])\n",
    "    for each in train_P:\n",
    "        new_p1[each] = 1\n",
    "    for each in test_P:\n",
    "        new_p2[each] = 1\n",
    "    #training matrix\n",
    "    training_list = []\n",
    "    for i,xi in enumerate(following):\n",
    "        x = (xi*new_p1).tolist()\n",
    "        training_list.append(x)\n",
    "    \n",
    "    \n",
    "    #testing matrix\n",
    "    testing_list = []\n",
    "    for i,xi in enumerate(following):\n",
    "        x = (xi*new_p2).tolist()\n",
    "        test_list.append(x)\n",
    "        \n",
    "    return np.array(training_list),np.array(testing_list)\n",
    "    pass\n",
    "\n",
    "\n",
    "X_tr, X_te = process(new_matrix, np.random.permutation(new_matrix.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(X, U, V):\n",
    "    \"\"\" Compute the mean error of the observed ratings in X and their estimated values. \n",
    "        Args: \n",
    "            X (numpy 2D array) : a ratings matrix as specified above\n",
    "            U (numpy 2D array) : a matrix of features for each user\n",
    "            V (numpy 2D array) : a matrix of features for each movie\n",
    "        Returns: \n",
    "            (float) : the mean squared error of the observed ratings with their estimated values\n",
    "        \"\"\"\n",
    "    dif =np.square(X- U.dot(V.T)) \n",
    "    new_dif =[X!=0]*dif\n",
    "    return np.mean(new_dif)\n",
    "    pass\n",
    "\n",
    "def train(X, X_te, k, U, V, niters=51, lam=10, verbose=False):\n",
    "    \"\"\" Train a collaborative filtering model. \n",
    "        Args: \n",
    "            X (numpy 2D array) : the training ratings matrix as specified above\n",
    "            X_te (numpy 2D array) : the testing ratings matrix as specified above\n",
    "            k (int) : the number of features use in the CF model\n",
    "            U (numpy 2D array) : an initial matrix of features for each user\n",
    "            V (numpy 2D array) : an initial matrix of features for each movie\n",
    "            niters (int) : number of iterations to run\n",
    "            lam (float) : regularization parameter\n",
    "            verbose (boolean) : verbosity flag for printing useful messages\n",
    "            \n",
    "        Returns:\n",
    "            (U,V) : A pair of the resulting learned matrix factorization\n",
    "    \"\"\"\n",
    "    temp = X !=0\n",
    "    W = temp.astype(np.int)\n",
    "    for ite in range(niters):\n",
    "        for j,w in enumerate(W): \n",
    "            U[j]=np.linalg.solve(V.T.dot((V.T.dot(np.diag(w))).T) + lam * np.eye(k), V.T.dot(X[j]))\n",
    "        for j,wt in enumerate(W.T):\n",
    "            V[j] = np.linalg.solve(U.T.dot(np.diag(wt).dot(U))+lam *np.eye(k),U.T.dot(X[:,j]))\n",
    "        if verbose == True:\n",
    "            if ite== 0:\n",
    "                print \"Iter |Train Err |Test Err\"\n",
    "            train_error= error(X, U, V)\n",
    "            test_error = error(X_te, U, V)\n",
    "            print ite, \"|\",train_error,\"|\",test_error\n",
    "            \n",
    "    return U,V\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U = np.random.rand(X_tr.shape[0],10)\n",
    "V = np.random.rand(X_tr.shape[1],10)\n",
    "U,V = train(X_tr, X_te, 10, U, V, niters=10, lam=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team got the following output: \n",
    "```python\n",
    "\n",
    "Iter |Train Err |Test Err\n",
    "0 | 0.0215557892728 | 0.0247727272727\n",
    "1 | 0.0124878550122 | 0.0247727272727\n",
    "2 | 0.011533655087 | 0.0247727272727\n",
    "3 | 0.0113421192911 | 0.0247727272727\n",
    "4 | 0.0112910212752 | 0.0247727272727\n",
    "5 | 0.011275019604 | 0.0247727272727\n",
    "6 | 0.0112696053164 | 0.0247727272727\n",
    "7 | 0.0112677039911 | 0.0247727272727\n",
    "8 | 0.0112670189888 | 0.0247727272727\n",
    "9 | 0.0112667645063 | 0.0247727272727\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Observations and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the result, the training errors decrease as the algorithm iterates, and is always smaller than the testing error. However, the testing error doesn’t seem to change at all, which might imply that our training data is not enough, or that using CF to extract hidden features might not be such a great idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that user tweets contain a lot of information and might be used to predict whether a user is following Obama. For example, when a user's tweets convey his or her interest in the field of politics, it would be reasonable to say that the user is more likely to have followed Obama. With this assumption in mind, we decided to use the bag-of-words model. Training data would be collective tweets from followers and non-followers of Obama respectively, and each testing instance would just be collective tweets from a specific user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Collecting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the rate limits on Twitter API calls, we spent quite some time collecting the amount of data that we thought we needed for the project. \n",
    "\n",
    "For postive examples (tweets from users who follow Obama), we simply collected the most recent 2000 followers of Obama. However, we found that many of the users have very few tweets, which might imply that they are fairly new Twitter users (also considering the fact that they just started following Obama). In order to keep the randomness in our selection of users rather than having a bias towards new users, we only collected tweets from those users who have more than 50 tweets. We collected 200 tweets from each \"qualified\" user, or however tweets that user has, if the count is less than 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get most recent 2000 followers of Obama\n",
    "followers = api.followers_ids(\"BarackObama\")[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "def get_tweets(screen_name, subdirectory):\n",
    "    \"\"\" Retrieve and save 200 tweets (or all tweets if less than 200) from the user given screen name, \n",
    "    if and only if the user has more than 50 total tweets\"\"\"\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(subdirectory)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try: \n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "        if len(new_tweets)>50:\n",
    "            outtweets = [tweet.text.encode(\"utf-8\") for tweet in new_tweets]\n",
    "            tweets_200 = ' '.join(outtweets)\n",
    "            with open(os.path.join(subdirectory, '%s_tweets.txt' % screen_name), 'a') as f:\n",
    "                f.write(tweets_200)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except tweepy.TweepError:\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of Boolean values, indicating whether the user has more than 50 tweets therefore whose tweets are saved\n",
    "tweet_saved = [] \n",
    "\n",
    "for follower in followers:\n",
    "    screen_name = api.get_user(follower).screen_name\n",
    "    tweet_saved.append(get_tweets(follower, \"obama_followers_tweets\"))\n",
    "\n",
    "print len(tweet_saved == True) # print count of valid followers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find users who do not follow Obama, we inspected the followers of the previous group of 2000 users who follow Obama. We think this is a good way to maintain the randomness in our training and test data. Specifically, we select the most recent follower of each obama-follower, check whether a) he or she is following Obama b) he or she has more than 50 tweets. Only when both a) and b) are met do we save the tweets from the specific user. Twitter does not have any endpoints for checking the existence of friendship between two users, therefore we retrieved the entire friend list of any user and checked whether Obama is in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get one follower from each of the previous 2000 followers of Obama\n",
    "candidates = [retrieve_n_recent_follower_ids(follower, 1) for follower in followers]\n",
    "\n",
    "print len(candidates) # should print 2000\n",
    "print candidates[:20] # print some IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isFollowing(users, target_user):\n",
    "    \"\"\"Given the twitter ID of a list of users and a target user screen name, return a list of Boolean values. \n",
    "    The value at specific index of returned vector corresponds to the fact of whether the user at the same index \n",
    "    follows the target user. \"\"\"\n",
    "    \n",
    "    isFollowing = []\n",
    "    for user in users:\n",
    "        try:\n",
    "            following = api.friends_ids(user)\n",
    "            if target_user in following:\n",
    "                isFollowing.append(1)\n",
    "            else:\n",
    "                isFollowing.append(0)\n",
    "        except tweepy.TweepError:\n",
    "            print \"sleep\"\n",
    "            time.sleep(60*15)\n",
    "            print \"wakeup\"\n",
    "            following = api.friends_ids(user)\n",
    "            if target_user in following:\n",
    "                isFollowing.append(1)\n",
    "            else:\n",
    "                isFollowing.append(0)\n",
    "    return isFollowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of Boolean values, indicating whether the candidate is following Obama\n",
    "is_obama_follower = isFollowing(candidates, obama_id)\n",
    "print len(is_obama_follower == 0) # number of non-followers (not necessarily with >50 tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get tweets from the non-followers\n",
    "tweet_saved = []\n",
    "for i, candidate in enumerate(candidates):\n",
    "    screen_name = api.get_user(candidate).screen_name\n",
    "    if (is_obama_follower[i] == 0):\n",
    "        tweet_saved.append(get_tweets(screen_name, \"obama_non_followers_tweets\"))\n",
    "        \n",
    "print len(tweet_saved == 1) # should print count of valid non-followers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Processing tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discovered that, the raw tweets we got from both follower and non-follower groups might contain contents that we need to take care of before the tweets could be used for \"bag-of-words\" model, including non-English words, punctuations, emoticons, hash-tags, etc. To be more specific, our preprocess procedure includes:\n",
    "\n",
    "1. remove stop words, including some customized stopwords\n",
    "2. remove punctuations\n",
    "3. remove non ascii characters\n",
    "4. remove tweets that are not written in English\n",
    "5. remove non words, such as \"ab123\"\n",
    "6. remove words with length equals 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rake\n",
    "import operator\n",
    "import stop_words\n",
    "import gensim\n",
    "import glob\n",
    "import enchant\n",
    "import random\n",
    "import collections\n",
    "from guess_language import guessLanguage\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = get_stop_words('en') # create English stop words list\n",
    "d = enchant.Dict(\"en_US\")\n",
    "input_files = glob.glob('obama_non_followers_tweets/*.txt')\n",
    "custom_stop_words = [\"just\", \"like\", \"can\", \"video\", \"new\", \"love\", \"via\", \"will\", \"one\", \"now\",\n",
    "                     \"know\", \"see\", \"get\", \"don\", \"today\", \"people\", \"day\", \"time\", \"make\"]\n",
    "\n",
    "\n",
    "def process_all(fnames, label):\n",
    "    for cnt, fname in enumerate(fnames):\n",
    "        sample_file = open(fname, 'r')\n",
    "        text = sample_file.read()\n",
    "\n",
    "        # tokenize\n",
    "        raw = text.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "        # remove non ascii str\n",
    "        tokens = map(remove_non_ascii, tokens)\n",
    "        tokens = filter(bool, tokens)\n",
    "        tokens = filter(lambda x: len(x) > 2, tokens)\n",
    "\n",
    "        # remove stop words\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        stopped_tokens = [i for i in stopped_tokens if not i in custom_stop_words]\n",
    "\n",
    "        texts = [i for i in stopped_tokens]\n",
    "        processed_texts = str(' '.join(texts))\n",
    "\n",
    "        if str(guessLanguage(processed_texts)) != 'en':\n",
    "            continue\n",
    "\n",
    "        # remove non english words\n",
    "        processed_texts_en = []\n",
    "        for tok in processed_texts.split():\n",
    "            if d.check(tok):\n",
    "                processed_texts_en.append(tok)\n",
    "        processed_texts_en = ' '.join(processed_texts_en)\n",
    "\n",
    "        if len(processed_texts_en.split()) < 100:\n",
    "            continue\n",
    "        tag = label\n",
    "        content = str(cnt) + '\\t' + tag + '\\t' + processed_texts_en + '\\n'\n",
    "        \n",
    "        filename = \"OBAMA_\" + label + \".txt\"\n",
    "        with open(filename, \"a\") as myfile:\n",
    "            myfile.write(content)\n",
    "\n",
    "        if cnt % 100 == 0:\n",
    "            print str(cnt) + \" round completed....\"\n",
    "\n",
    "\n",
    "            \n",
    "input_files = glob.glob('obama_non_followers_tweets/*.txt')\n",
    "process_all(input_files, 'FALSE')\n",
    "input_files = glob.glob('obama_followers_tweets/*.txt')\n",
    "process_all(input_files, 'TRUE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Learning and predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merged the preprocessed tweets into two big corpus, the follower tweets and the non-follower tweets. We used Naive Bayes and Maxiumn a Posteriori Probability (MAP) for the classifier, where we calculated the frequency for each word in both corpus, with Laplace smoothing. During the testing phase, when given the collective tweets of a user, we calculate the probability of following and not following, and classify the user based on the one more likely. \n",
    "\n",
    "**Because part of the code we used for this section is from one of the team member's homework for another class, we will not post the code here, but submit it through Autolab instead. **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We carefully selected training and test set such that both set have about the same size. We varied the size of training and test dataset to see how the model perform on different sizes. For each different size, we ran the model three times with randomly selected training and test set from our pool, and the results are as follows:\n",
    "\n",
    "```\n",
    "collective tweets from obama-followers train|test|#collective tweets from non-obama-followers train|test \n",
    "--> (run_1_result, run_2_result, run_3_result) \n",
    "\n",
    "253 50 295 50 --> (0.77, 0.84, 0.77)\n",
    "200 50 250 50 --> (0.74, 0.72, 0.74)\n",
    "150 50 200 50 --> (0.72, 0.72, 0.70)\n",
    "100 50 150 50 --> (0.72, 0.67, 0.71)\n",
    "50 50 100 50 --> (0.77, 0.74, 0.71)\n",
    "50 50 50 50 --> (0.56, 0.63, 0.67)\n",
    "25 50 25 50 --> (0.68, 0.57, 0.67)\n",
    "10 50 10 50 --> (0.58, 0.61, 0.61)\n",
    "```\n",
    "\n",
    "As we could see, the average prediction accuracy increased as we used more training data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Observations and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had some interesting observations after further analyzing both the result and the data. We also tried to modify our model in a couple of ways to see whether its performance would improve. \n",
    "\n",
    "a ) We started with the default nltk stop word set, but later when we inspected the most frequent words from the two corpus, we found that there were still a lot of overlapping between the two lists of top words, which did not necessarily seem relevant to whether the user would actually follow Barack Obama. Since the idea for \"bag-of-words\" text classification was that the corpus under each class label should have rather \"characteristic\" words, meaning that they should be specific to that label, we decided to add those words to the stop word set. \n",
    "\n",
    "\n",
    "Some of the results of comparing the model with and without customized stop words:\n",
    "```\n",
    "200 50 250 50 \n",
    "--> (0.74, 0.72, 0.74) with customized stopwords\n",
    "--> (0.61, 0.51, 0.61) without customized stopwords\n",
    "\n",
    "253 50 295 50 \n",
    "--> (0.77, 0.84, 0.77) with customized stopwords \n",
    "--> (0.62, 0.52, 0.61) without customized stopwords\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "b) The accuracy seems to have something to do with the training set we chose to use. When using a different subset of all training data we got as the training set while keeping the test set the same, the results might vary a bit (by around 10%). We suspected that this was caused by the fact that a user's tweets sometimes might not accurately reflect the friendship status between the user and Obama. To use a more straightforward example, if a user does not talk about politics on Twitter, that does not necessarily suggest that the user would not follow Obama, and vice versa. In those cases, the tweets can only get the model more confused by adding irrelevant words to the corpus, while not actually helping the prediction. When the subset contains tweets that have stronger relevance to the classification (say, when contains a lot of keywords from the field of politics), the model may tend to perform better on test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realized that we could've possibly chosen another Twitter celebrity for this project such that the model would probably have better performance. If we had used Katy Perry, for instance, it's reasonable to believe that Twitter followers of Katy Perry will probably tweet more about music comparing to Twitter followers of Obama tweet about politics.\n",
    "\n",
    "Some other features we think might be helpful, if we had more time to work on this model, would be \n",
    "1. How many of my followers have followed Obama, and\n",
    "2. How many of my friends(people I'm following) are \"similar\" to Obama (maybe are also politicians in this case)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
