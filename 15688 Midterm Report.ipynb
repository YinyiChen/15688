{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, the team looks into predicting whether a given user is likely to follow a specific celebrity on twitter who the user hasn’t followed yet. The result could be used to automatically generate recommended celebrities for users. Furthermore, we are interested how to extend the model to also predict the likelihood of a user following another ordinary user.\n",
    "\n",
    "\n",
    "## General Approach\n",
    "\n",
    "The substantial amount of data available through Twitter API gives a lot of flexibility in selecting data, processing data, selecting learning model, etc. So far, we have thoughts on using the following approaches:\n",
    "\n",
    "### 1. Collaborate filtering and alternating least squares method\n",
    "\n",
    "The team found that the behavior of a user following another user is in some way similar to how movie ratings happen for MovieLens dataset. The difference is that, instead of a floating number from 1 to 5 for a rating, the unidirectional relationship of follow/not follow only has a binary value. It would be interesting to observe how this property would affect the performance of alternating least squares method when it’s applied. And as a possible way to close the gap between the two scenarios, a “fondness” score could be added for a follower to one of the target he or she is following, indicating how much of a fan the follower is to the target. For example, if a user has retweeted or liked a lot of posts of the target, the fondness score might then be relatively high.\n",
    "\n",
    "### 2. Neural network \n",
    "\n",
    "Rather than implicit features used in alternating least square method, the team also wants to try to manually select and/or extract features from twitter data that might seem to play a part in the prediction.\n",
    "\n",
    "### 3. Linear Regression and SVM\n",
    "\n",
    "The team also plans to use Natural Language Processing to analysis each user's tweets, and generate a feature vector for each user. The team planned to use \"monkeylearn\" to do text analysis. After generating user's feature vector, then the team plan to use Linear Regression and SVM model to do the prediction.\n",
    "\n",
    "\n",
    "\n",
    "#### The team has so far been working on method 1 only, and would like to give a walk-through of our work in this report.\n",
    "\n",
    "\n",
    "## Finding top 100 most popular Twitter accounts\n",
    "\n",
    "We start from creating a developer account on Twitter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from twython import Twython\n",
    "\n",
    "\n",
    "consumer_key = '<CONSUMER_KEY>'\n",
    "consumer_secret = '<CONSUMER_SECRET>'\n",
    "access_token = '<ACCESS_TOKEN>'\n",
    "access_secret = '<ACCESS_SECRET>'\n",
    "auth = OAuthHandler(consumer_key=consumer_key, consumer_secret=consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For this project, mainly four \"objects\" are retrieved from Twitter API: “Tweets”,” Users”, “Entities”, and “Places”. The team queried data from “Users” and “Tweets”. When querying data from Twitter API, the biggest problem that the team faced was the limitation on the query rate. Rate limits are divided into 15 minute intervals and the team can only call the same API endpoint 15 time per 15 minutes. Hence, the team has been using a small data set for this project. \n",
    "\n",
    "First,the team chose 100 popular twitter accounts based on Twitter statistics and used BeautifulSoup to get a list these 100 accounts’ screen_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup library imports\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "web = \"http://twittercounter.com/pages/100?vt=1&utm_expid=102679131-111.l9w6V73qSUykZciySuTZuA.1&utm_referrer=https%3A%2F%2Fwww.google.com%2F\"\n",
    "\n",
    "page = urllib2.urlopen(web)\n",
    "soup = BeautifulSoup(page)\n",
    "span_list = soup.find_all(\"span\", {\"itemprop\":\"alternateName\"})\n",
    "name_list = []\n",
    "for each in span_list:\n",
    "    name_list.append(str(each.text)[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code prints out the following list of top 100 Twitter celebrities:\n",
    "\n",
    "    ['katyperry', 'justinbieber', 'taylorswift13', 'BarackObama', 'rihanna', 'YouTube', 'ladygaga', 'TheEllenShow', 'twitter', 'jtimberlake', 'KimKardashian', 'britneyspears', 'Cristiano', 'selenagomez', 'cnnbrk', 'jimmyfallon', 'ArianaGrande', 'shakira', 'instagram', 'ddlovato', 'JLo', 'Oprah', 'Drake', 'KingJames', 'BillGates', 'nytimes', 'onedirection', 'KevinHart4real', 'MileyCyrus', 'SportsCenter', 'espn', 'CNN', 'Harry_Styles', 'Pink', 'LilTunechi', 'wizkhalifa', 'NiallOfficial', 'Adele', 'BrunoMars', 'BBCBreaking', 'kanyewest', 'neymarjr', 'KAKA', 'ActuallyNPH', 'danieltosh', 'narendramodi', 'aliciakeys', 'NBA', 'LiamPayne', 'Louis_Tomlinson', 'SrBachchan', 'EmmaWatson', 'pitbull', 'khloekardashian', 'iamsrk', 'ConanOBrien', 'kourtneykardash', 'realmadrid', 'Eminem', 'davidguetta', 'NICKIMINAJ', 'NFL', 'AvrilLavigne', 'KendallJenner', 'BeingSalmanKhan', 'zaynmalik', 'NASA', 'aamir_khan', 'FCBarcelona', 'KylieJenner', 'blakeshelton', 'chrisbrown', 'coldplay', 'aplusk', 'TheEconomist', 'vine', 'MariahCarey', 'BBCWorld', 'LeoDiCaprio', 'edsheeran', 'deepikapadukone', 'google', 'xtina', 'MohamadAlarefe', 'agnezmo', 'shugairi', 'ricky_martin', 'TwitterEspanol', 'priyankachopra', 'realDonaldTrump', 'Reuters', 'JimCarrey', 'iHrithik', 'KDTrey5', 'RyanSeacrest', 'ivetesangalo', 'akshaykumar', 'AlejandroSanz', 'SnoopDogg', 'TwitterSports']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Followers From Each Celebrity User\n",
    "\n",
    "Next step is to iterate over these 100 popular accounts’ followers and fetch n followers(“general user”) (use n=1 here for experimenting) from each popular accounts’ followers list. Those selected followers will together form the group of users for whom we'll be predicting the likelihood of following each of the 100 Twitter celebrities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def retrieve_n_follower_ids(id, n):\n",
    "    \"\"\" Retrieve n followers from a given Twitter id by taking the first user of the first n returning page\"\"\"\n",
    "    user = api.get_user(id)\n",
    "    print user.followers_count\n",
    "    followers = []\n",
    "    for i, follower in enumerate(tweepy.Cursor(api.followers_ids, id=id).pages()):\n",
    "        if i == n:\n",
    "            break\n",
    "        followers.append(follower[0])\n",
    "    return followers\n",
    "\n",
    "\n",
    "def generate_user_id_list (id_list,n):\n",
    "    \"\"\" Generate the entire set of users that unions the returned list of running the retrieve_n_follower_ids \n",
    "    for each celebrity account\"\"\"\n",
    "    \n",
    "    users = []\n",
    "    for id in id_list:\n",
    "        try:\n",
    "            new_users = retrieve_n_follower_ids(id, n)\n",
    "        except tweepy.TweepError:\n",
    "            time.sleep(60*15)\n",
    "            new_users = retrieve_n_follower_ids(id, n)\n",
    "        users += new_users\n",
    "        \n",
    "    return set(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = generate_user_id_list (name_list, 1)\n",
    "print users\n",
    "print len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the following outputs:\n",
    "\n",
    "    [800177281964486656, 800176882859638784, 800177281964486656, 300477987, 800171035093921792, 800176806456238080, 785276975455805441, 356142384, 3079691526, 1067137320, 3628005984, 800177747666440192, 800173059671887872, 800177281964486656, 800177347777306624, 800180452086550529, 800174055315808256, 800180795377758208, 786644749067358208, 800175512131751937, 800122422988943360, 800180659255799810, 800177487686737921, 800178652478476288, 800172515234455552, 800180926483337218, 800179509139881985, 1404269034, 704775480017362944, 800181164510019584, 800184113520967680, 800183470802628608, 719360741606694912, 800184964876627968, 800185366942601216, 203195412, 800185355341152257, 791159574162276353, 800150996882051073, 800184775445073920, 203195412, 800185192577019904, 800184420887990272, 800173538212577280, 800185302459387904, 800187424185851904, 3763069162, 800188092900462593, 720635469743022081, 720635469743022081, 799587973129965568, 800188741872586756, 790739389429059584, 791905771113938944, 744753183809998848, 799622713040113664, 141358236, 745695894666907648, 799842965418110977, 1945758307, 800071767532261376, 800192899069554689, 800191361395736576, 800192632685068288, 800189724925145088, 800192267545759745, 800190776726601728, 792704170092556288, 800187664628449280, 799082231075586062, 800190776726601728, 800191877513232385, 516222248, 112533047, 800192551399485441, 795516083323228160, 330074489, 800195958839443456, 330074489, 800196114972450820, 800196601939566594, 794328224104845312, 3222517458, 800192769293619200, 800195142619541504, 800192769293619200, 800196624723021824, 2842125299, 800196061016911872, 800196538089631744, 800199656256860161, 3131642635, 800197015154008064, 800198019803025408, 800200273796829184, 798796801058816000, 800199539558739968, 800199578326683648, 800200401794404357, 800197419484868608]\n",
    "    \n",
    "100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate user-celebrity matrix\n",
    "\n",
    "Now that both user and celebrity arrays are ready, the following step is to generate the m by n matrix (m is number of total general users and n the number of celebrities collected). matrix[i][j] indicates whether user i is currently following celebrity j. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def genenerate_user_celebrity_matrix(users, celebrities): # both are lists of Twitter IDs\n",
    "    matrix = np.zeros((len(users), len(celebrities)))\n",
    "    celebrity_set = set(celebrities)\n",
    "    for i in range(len(users)):\n",
    "        try:\n",
    "            friends = get_friends(users[i]) \n",
    "        except tweepy.TweepError:\n",
    "            time.sleep(60*15)\n",
    "            friends = get_friends(users[i])    \n",
    "        for j in range(len(celebrities)):\n",
    "            matrix[i][j] = celebrities[j] in friends\n",
    "    return matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "celebrities = id_list\n",
    "matrix = genenerate_user_celebrity_matrix(users, celebrities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team is able to get the following outputs:\n",
    "```python\n",
    "user_id = 800177281964486656\n",
    "user_id = 800176882859638784\n",
    "user_id = 800177281964486656\n",
    "user_id = 300477987\n",
    "user_id = 800171035093921792\n",
    "user_id = 800176806456238080\n",
    "user_id = 785276975455805441\n",
    "user_id = 3079691526\n",
    "user_id = 1067137320\n",
    "user_id = 3628005984\n",
    "user_id = 800177747666440192\n",
    "user_id = 800173059671887872\n",
    "user_id = 800177281964486656\n",
    "user_id = 800177347777306624\n",
    "user_id = 800180452086550529\n",
    "user_id = 800174055315808256\n",
    "sleep\n",
    "wake up\n",
    "user_id = 800174055315808256\n",
    "user_id = 800180795377758208\n",
    "user_id = 786644749067358208\n",
    "user_id = 800175512131751937\n",
    "user_id = 800122422988943360\n",
    "user_id = 800180659255799810\n",
    "user_id = 800177487686737921\n",
    "user_id = 800178652478476288\n",
    "user_id = 800172515234455552\n",
    "user_id = 800180926483337218\n",
    "user_id = 800179509139881985\n",
    "user_id = 1404269034\n",
    "user_id = 704775480017362944\n",
    "user_id = 800181164510019584\n",
    "user_id = 800184113520967680\n",
    "user_id = 800183470802628608\n",
    "sleep\n",
    "wake up\n",
    "user_id = 800183470802628608\n",
    "user_id = 719360741606694912\n",
    "user_id = 800184964876627968\n",
    "user_id = 800185366942601216\n",
    "user_id = 203195412\n",
    "user_id = 800185355341152257\n",
    "user_id = 791159574162276353\n",
    "user_id = 800150996882051073\n",
    "user_id = 800184775445073920\n",
    "user_id = 203195412\n",
    "user_id = 800185192577019904\n",
    "user_id = 800184420887990272\n",
    "user_id = 800173538212577280\n",
    "user_id = 800185302459387904\n",
    "user_id = 800187424185851904\n",
    "user_id = 3763069162\n",
    "sleep\n",
    "wake up\n",
    "user_id = 3763069162\n",
    "user_id = 800188092900462593\n",
    "user_id = 720635469743022081\n",
    "user_id = 720635469743022081\n",
    "user_id = 799587973129965568\n",
    "user_id = 800188741872586756\n",
    "user_id = 790739389429059584\n",
    "user_id = 791905771113938944\n",
    "user_id = 744753183809998848\n",
    "user_id = 745695894666907648\n",
    "user_id = 799842965418110977\n",
    "user_id = 1945758307\n",
    "user_id = 800071767532261376\n",
    "user_id = 800192899069554689\n",
    "user_id = 800191361395736576\n",
    "user_id = 800192632685068288\n",
    "sleep\n",
    "wake up\n",
    "user_id = 800192632685068288\n",
    "user_id = 800189724925145088\n",
    "user_id = 800192267545759745\n",
    "user_id = 800190776726601728\n",
    "user_id = 792704170092556288\n",
    "user_id = 800187664628449280\n",
    "user_id = 799082231075586062\n",
    "user_id = 800190776726601728\n",
    "user_id = 800191877513232385\n",
    "user_id = 516222248\n",
    "user_id = 112533047\n",
    "user_id = 800192551399485441\n",
    "user_id = 795516083323228160\n",
    "user_id = 330074489\n",
    "user_id = 800195958839443456\n",
    "sleep\n",
    "wake up\n",
    "user_id = 800195958839443456\n",
    "user_id = 330074489\n",
    "user_id = 800196114972450820\n",
    "user_id = 800196601939566594\n",
    "user_id = 794328224104845312\n",
    "user_id = 3222517458\n",
    "user_id = 800192769293619200\n",
    "user_id = 800195142619541504\n",
    "user_id = 800192769293619200\n",
    "user_id = 800196624723021824\n",
    "user_id = 2842125299\n",
    "user_id = 800196061016911872\n",
    "user_id = 800196538089631744\n",
    "user_id = 800199656256860161\n",
    "user_id = 800197015154008064\n",
    "user_id = 800198019803025408\n",
    "sleep\n",
    "wake up\n",
    "user_id = 800198019803025408\n",
    "user_id = 800200273796829184\n",
    "user_id = 798796801058816000\n",
    "user_id = 800199539558739968\n",
    "user_id = 800199578326683648\n",
    "user_id = 800200401794404357\n",
    "user_id = 800197419484868608\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#delete all zero users\n",
    "new_matrix = []\n",
    "for i in range(matrix.shape[0]):\n",
    "    if np.count_nonzero(matrix[i])!= 0:\n",
    "        new_matrix.append(matrix[i])\n",
    "new_matrix = np.array(new_matrix)\n",
    "print new_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team got the following outputs:\n",
    "```python\n",
    "(88, 100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Using Collaborate filtering do analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Doing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def process(following, P):\n",
    "    \"\"\" Given a dataframe of following and a random permutation, split the data into a training \n",
    "        and a testing set, in matrix form. \n",
    "        \n",
    "        Args: \n",
    "            following (2D numpy array) : a 2D numpy array of following \n",
    "            P (numpy 1D array) : random permutation vector\n",
    "            \n",
    "        Returns: \n",
    "            (X_tr, X_te)  : training and testing splits of the following matrix (both \n",
    "                                         numpy 2D arrays) \n",
    "    \"\"\"\n",
    "    l= len(following)   \n",
    "    train_length = int(math.floor(l*0.9))\n",
    "    train_P = P[0:train_length]\n",
    "    test_P = P[train_length:]\n",
    "    \n",
    "    new_p1 = np.zeros(following.shape[1])\n",
    "    new_p2= np.zeros(following.shape[1])\n",
    "    for each in train_P:\n",
    "        new_p1[each] = 1\n",
    "    for each in test_P:\n",
    "        new_p2[each] = 1\n",
    "    #training matrix\n",
    "    training_list = []\n",
    "    for i,xi in enumerate(following):\n",
    "        x = (xi*new_p1).tolist()\n",
    "        training_list.append(x)\n",
    "    \n",
    "    \n",
    "    #testing matrix\n",
    "    testing_list = []\n",
    "    for i,xi in enumerate(following):\n",
    "        x = (xi*new_p2).tolist()\n",
    "        test_list.append(x)\n",
    "        \n",
    "    return np.array(training_list),np.array(testing_list)\n",
    "    pass\n",
    "\n",
    "X_tr, X_te, movieNames = process(ratings, movies, np.random.permutation(len(ratings)))\n",
    "# print X_tr[0][np.nonzero(X_tr[0])]\n",
    "# print movieNames[:5]\n",
    "print X_tr.shape, X_te.shape,movieNames[:5]\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr, X_te = process(new_matrix, np.random.permutation(new_matrix.shape[1]))\n",
    "print X_tr.shape\n",
    "print X_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team got the following output:  \n",
    "```python\n",
    "(88, 100)\n",
    "(88, 100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Calculate U and V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(X, U, V):\n",
    "    \"\"\" Compute the mean error of the observed ratings in X and their estimated values. \n",
    "        Args: \n",
    "            X (numpy 2D array) : a ratings matrix as specified above\n",
    "            U (numpy 2D array) : a matrix of features for each user\n",
    "            V (numpy 2D array) : a matrix of features for each movie\n",
    "        Returns: \n",
    "            (float) : the mean squared error of the observed ratings with their estimated values\n",
    "        \"\"\"\n",
    "    dif =np.square(X- U.dot(V.T)) \n",
    "    new_dif =[X!=0]*dif\n",
    "    return np.mean(new_dif)\n",
    "    pass\n",
    "\n",
    "def train(X, X_te, k, U, V, niters=51, lam=10, verbose=False):\n",
    "    \"\"\" Train a collaborative filtering model. \n",
    "        Args: \n",
    "            X (numpy 2D array) : the training ratings matrix as specified above\n",
    "            X_te (numpy 2D array) : the testing ratings matrix as specified above\n",
    "            k (int) : the number of features use in the CF model\n",
    "            U (numpy 2D array) : an initial matrix of features for each user\n",
    "            V (numpy 2D array) : an initial matrix of features for each movie\n",
    "            niters (int) : number of iterations to run\n",
    "            lam (float) : regularization parameter\n",
    "            verbose (boolean) : verbosity flag for printing useful messages\n",
    "            \n",
    "        Returns:\n",
    "            (U,V) : A pair of the resulting learned matrix factorization\n",
    "    \"\"\"\n",
    "    temp = X !=0\n",
    "    W = temp.astype(np.int)\n",
    "    for ite in range(niters):\n",
    "        for j,w in enumerate(W): \n",
    "            U[j]=np.linalg.solve(V.T.dot((V.T.dot(np.diag(w))).T) + lam * np.eye(k), V.T.dot(X[j]))\n",
    "        for j,wt in enumerate(W.T):\n",
    "            V[j] = np.linalg.solve(U.T.dot(np.diag(wt).dot(U))+lam *np.eye(k),U.T.dot(X[:,j]))\n",
    "        if verbose == True:\n",
    "            if ite== 0:\n",
    "                print \"Iter |Train Err |Test Err\"\n",
    "            train_error= error(X, U, V)\n",
    "            test_error = error(X_te, U, V)\n",
    "            print ite, \"|\",train_error,\"|\",test_error\n",
    "            \n",
    "    return U,V\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U = np.random.rand(X_tr.shape[0],10)\n",
    "V = np.random.rand(X_tr.shape[1],10)\n",
    "U,V = train(X_tr, X_te, 10, U, V, niters=10, lam=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team got the following output: \n",
    "```python\n",
    "\n",
    "Iter |Train Err |Test Err\n",
    "0 | 0.0215557892728 | 0.0247727272727\n",
    "1 | 0.0124878550122 | 0.0247727272727\n",
    "2 | 0.011533655087 | 0.0247727272727\n",
    "3 | 0.0113421192911 | 0.0247727272727\n",
    "4 | 0.0112910212752 | 0.0247727272727\n",
    "5 | 0.011275019604 | 0.0247727272727\n",
    "6 | 0.0112696053164 | 0.0247727272727\n",
    "7 | 0.0112677039911 | 0.0247727272727\n",
    "8 | 0.0112670189888 | 0.0247727272727\n",
    "9 | 0.0112667645063 | 0.0247727272727\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the result, the training errors decrease as the algorithm iterates, and is always smaller than the testing error. However, the testing error doesn’t change at all, which might imply that our training data is not enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
